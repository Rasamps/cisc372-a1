{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTs11WubmRhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#R: Comments beginning with \"R:\" are added by me for the purpose of the assignment.\n",
        "#R: Brings both the train and test data sets into our \"local\" folder for use.\n",
        "# download data (-q is the quiet mode)\n",
        "! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv\n",
        "! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrsga6qkouO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#R: Import pandas, a Python library used in data manipulation and analysis.\n",
        "import pandas as pd\n",
        "\n",
        "#R: Importing the train data set using pandas, we partition it into two variables;\n",
        "#   one containing all the descriptive attributes (X_train --> bedrooms, accommodates, etc.) \n",
        "#   and the other the attribute to be predicted (y_train --> price_rating).\n",
        "Xy_train = pd.read_csv('train.csv', engine='python')\n",
        "X_train = Xy_train.drop(columns=['price_rating'])\n",
        "y_train = Xy_train[['price_rating']]\n",
        "\n",
        "#R: Prints how many training records we have.\n",
        "print('training', len(X_train))\n",
        "#R: Creates a histogram of the 'price_rating' values found in the training set.\n",
        "#   Shows the number of records in each target class label.\n",
        "Xy_train.price_rating.hist()\n",
        "\n",
        "#R: Loads in the records of the testing set. Each object in the test\n",
        "#   set lacks a price_rating value. This is what we attempt to predict.\n",
        "X_test = pd.read_csv('test.csv', engine='python')\n",
        "\n",
        "#R: Attaches the Id of each \"new\" listing to a variable.\n",
        "testing_ids = X_test.Id\n",
        "\n",
        "#R: Prints how many test records we have.\n",
        "print('testing', len(X_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFDJ3lNmE6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model training and tuning\n",
        "#R: Import numerous libraries used for data manipulation, analysis and to build\n",
        "#   machine learning models capable of best predicting the price_rating \n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "#R: A random seed makes it so each run of the script produces the same results.\n",
        "np.random.seed(0)\n",
        "\n",
        "#R: Define which numeric attributes in our training set we would like to use to train our model with.\n",
        "numeric_features = ['bedrooms', 'review_scores_location', 'accommodates', 'beds']\n",
        "\n",
        "#R: The numeric_transformer is a way of processing our numeric data...\n",
        "#   The Pipeline takes a number of transformations / processing methods and applies them to the data.\n",
        "#   SimpleImputer predicts missing values using the median of that attributes known data.\n",
        "#   StandardScaler scales the data to have a standard normal distribution.\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "#R: Define which categorical attributes in our training set we would like to use to train our model with.\n",
        "categorical_features = ['property_type', 'is_business_travel_ready', 'room_type']\n",
        "\n",
        "#R: Similar to the numeric_transformer, the categorical_transformer is a way of preprocessing our data.\n",
        "#   SimpleImputer in this case fills missing values with the string 'missing'.\n",
        "#   OneHotEncoder is a way of processing categorical data into a one-hot numeric array. \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "#R: Creates a variable which holds both our attribute transformers.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "#R: Creates a variable holding our two transformers as well as specifying what type of machine learning model\n",
        "#   we would like to use. In this sample script it is a XGBoost for classification. For the chosen model\n",
        "#   two default hyperparameters are set.\n",
        "regr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', XGBClassifier(\n",
        "                          objective='multi:softmax', seed=1))])\n",
        "\n",
        "#R: From the full training and test datasets, partition out the columns which pertain to the attributes\n",
        "#   we use for training and classification.\n",
        "X_train = X_train[[*numeric_features, *categorical_features]]\n",
        "X_test = X_test[[*numeric_features, *categorical_features]]\n",
        "\n",
        "# `__` denotes attribute \n",
        "# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`\n",
        "#  which is our xgb)\n",
        "#R: Create a grid of possible hyperparameter values. Each combination in this grid will be evaluated.\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean'],\n",
        "    'regressor__n_estimators': [50, 100],\n",
        "    'regressor__max_depth':[10, 20]\n",
        "}\n",
        "#R: Using the above hyperparameter grid, evaluate each combination. Use 5-fold cross validation.\n",
        "#   The scoring metric used is accuracy.\n",
        "grid_search = GridSearchCV(\n",
        "    regr, param_grid, cv=5, verbose=3, n_jobs=2, \n",
        "    scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#R: Prints the best score acheived from our hyperparameter search.\n",
        "print('best score {}'.format(grid_search.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF6WrzdKmJ97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction & generating the submission file\n",
        "#R: Creates a csv file containing the Id for each test record as well as the newly\n",
        "#   predicted label for each of these Airbnb Id's.\n",
        "y_pred = grid_search.predict(X_test)\n",
        "pd.DataFrame(\n",
        "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}